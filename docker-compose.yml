# Kompletny system LLM Email Distribution
version: '3.8'

services:
  # LLM Code Generator Service
  llm-generator:
    build:
      context: ./llm-generator
      dockerfile: Dockerfile
    container_name: llm-generator
    environment:
      - LLM_PROVIDER=ollama  # openai, anthropic, ollama
      - OLLAMA_URL=http://ollama:11434  # Using the default Ollama port
      - REDIS_URL=redis://redis:6379
      - SMTP_SERVICE_URL=http://smtp-service:5000
    ports:
      - "${LLM_GENERATOR_PORT}"
    depends_on:
      - redis
      - ollama
    volumes:
      - ./templates:/app/templates
      - ./generated:/app/generated
    networks:
      - llm-network

  # Local LLM Service (Ollama)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-llm
    ports:
      - "${OLLAMA_PORT}"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - llm-network
    # Start the Ollama service and pull models in the background
    entrypoint: "/bin/sh"
    command: >
      -c "ollama serve &
          sleep 5 &&
          ollama pull codellama:7b-instruct &
          ollama pull mistral:7b-instruct &
          wait"

  # SMTP Distribution Service
  smtp-service:
    build:
      context: ./smtp-service
      dockerfile: Dockerfile
    container_name: smtp-distribution
    environment:
      - SMTP_HOST=${SMTP_HOST:-smtp.gmail.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USER=${SMTP_USER}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SMTP_USE_TLS=${SMTP_USE_TLS:-true}
      - REDIS_URL=redis://redis:6379
      - WEBHOOK_SECRET=${WEBHOOK_SECRET:-secret123}
    ports:
      - "${SMTP_SERVICE_PORT}"
    depends_on:
      - redis
    volumes:
      - ./email-templates:/app/templates
      - ./attachments:/app/attachments
    networks:
      - llm-network

  # Redis for job queue and caching
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    ports:
      - "6379:6379"  # Redis default port, not changed in .env
    volumes:
      - redis_data:/data
    networks:
      - llm-network

  # MailHog for local SMTP testing
  mailhog:
    image: mailhog/mailhog:latest
    container_name: mailhog-smtp
    ports:
      - "${MAILHOG_SMTP_PORT}"
      - "${MAILHOG_WEB_PORT}"  # Web UI
    networks:
      - llm-network

  # Webhook receiver for testing
  webhook-receiver:
    build:
      context: ./webhook-receiver
      dockerfile: Dockerfile
    container_name: webhook-receiver
    ports:
      - "${WEBHOOK_RECEIVER_PORT}"
    environment:
      - LLM_SERVICE_URL=http://llm-generator:8006
    networks:
      - llm-network

volumes:
  ollama_data:
  redis_data:

networks:
  llm-network:
    driver: bridge



